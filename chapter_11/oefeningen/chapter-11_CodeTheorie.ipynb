{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPxupplVZBwxSCwXa99IKf2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"xicTciQ7-UKR","executionInfo":{"status":"ok","timestamp":1705311727892,"user_tz":-60,"elapsed":7718,"user":{"displayName":"Floris Buyse","userId":"15555724134925172037"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["### Initialization in Keras"],"metadata":{"id":"cxFxTW_4-sQR"}},{"cell_type":"markdown","source":["Default = Glorot initialization with uniform distribution"],"metadata":{"id":"Bn7TxyMZFtIY"}},{"cell_type":"code","source":["dense = tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\") # or \"he_uniform\""],"metadata":{"id":"vUtdfk-I-gAB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Other activation functions"],"metadata":{"id":"KdBWfm1L_AqY"}},{"cell_type":"markdown","source":["`Leaky ReLU`\n","\n","Use He initialization with Leaky ReLU"],"metadata":{"id":"5sd7a_Sy_EBb"}},{"cell_type":"code","source":["leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2) # default = 0.3\n","dense = tf.keras.layers.Dense(50, activation=leaky_relu, kernel_initializer=\"he_normal\")"],"metadata":{"id":"2LN80JH3_Ca-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Or as a seperate layer"],"metadata":{"id":"_bcE8E8M_ZoE"}},{"cell_type":"code","source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"),\n","    tf.keras.layers.LeakyReLU(alpha=0.2)\n","])"],"metadata":{"id":"sCNN8tWs_bOT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`Swish`\n","\n","Also He initialization"],"metadata":{"id":"WCafjcfi_nWl"}},{"cell_type":"code","source":["dense = tf.keras.layers.Dense(50, activation=\"swish\", kernel_initializer=\"he_normal\")"],"metadata":{"id":"ecwxLgSj_oyq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Batch Normalization"],"metadata":{"id":"PUBLmPxbAQX7"}},{"cell_type":"markdown","source":["`Use after activation function`"],"metadata":{"id":"JxRDl4vXFqHG"}},{"cell_type":"code","source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=[28, 28]),\n","    tf.keras.layers.BatchNormalization(),\n","\n","    tf.keras.layers.Dense(300, activation=\"relu\",\n","                              kernel_initializer=\"he_normal\"),\n","    tf.keras.layers.BatchNormalization(),\n","\n","    tf.keras.layers.Dense(100, activation=\"relu\",\n","                              kernel_initializer=\"he_normal\"),\n","    tf.keras.layers.BatchNormalization(),\n","\n","    tf.keras.layers.Dense(10, activation=\"softmax\")\n","])"],"metadata":{"id":"x5luRIk0_3nm","executionInfo":{"status":"ok","timestamp":1705311728665,"user_tz":-60,"elapsed":782,"user":{"displayName":"Floris Buyse","userId":"15555724134925172037"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["`Parameter Calculation`\n","\n","BatchNorm adds 4 parameters:\n","\n","- Gamma, Beta = Learnable\n","- Mu, Sigma = Non-learnable\n","\n","In this example:\n","\n","- BN1: 784 inputs x 4 = 3136 parameters\n","- BN2: 300 x 4 = 1200 parameters\n","- BN3: 100 x 4 = 400 parameters\n","\n","Total = 4736 parameters\n","Total non-trainable = 4736 / 2 = 2368 parameters"],"metadata":{"id":"TqXlqfWXCMu8"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"a_grLYgWAmHO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705311740284,"user_tz":-60,"elapsed":494,"user":{"displayName":"Floris Buyse","userId":"15555724134925172037"}},"outputId":"5286c7db-ec73-4eea-9e42-b89d45204240"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten (Flatten)           (None, 784)               0         \n","                                                                 \n"," batch_normalization (Batch  (None, 784)               3136      \n"," Normalization)                                                  \n","                                                                 \n"," dense (Dense)               (None, 300)               235500    \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 300)               1200      \n"," chNormalization)                                                \n","                                                                 \n"," dense_1 (Dense)             (None, 100)               30100     \n","                                                                 \n"," batch_normalization_2 (Bat  (None, 100)               400       \n"," chNormalization)                                                \n","                                                                 \n"," dense_2 (Dense)             (None, 10)                1010      \n","                                                                 \n","=================================================================\n","Total params: 271346 (1.04 MB)\n","Trainable params: 268978 (1.03 MB)\n","Non-trainable params: 2368 (9.25 KB)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["`Use before activation function`\n","\n","use_bias=False -> learnable parameters Gamma and Beta from BatchNorm are in essence biases -> no need for extra bias from Dense layer"],"metadata":{"id":"PUhSLCCYA4gi"}},{"cell_type":"code","source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=[28, 28]),\n","    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.Activation(\"relu\"),\n","\n","    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.Activation(\"relu\"),\n","\n","    tf.keras.layers.Dense(10, activation=\"softmax\")\n","])"],"metadata":{"id":"l7hITXs8A7FW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`Hyperparameters of BN`\n","\n","- momentum: to compute moving averages\n","- axis: over which axis will mean and variance be computed"],"metadata":{"id":"h4y1PKzVEBp_"}},{"cell_type":"code","source":["BN = tf.keras.layers.BatchNormalization(momentum=0.99, axis=-1) # These are default settings # axis=[1,2] if batchshape = (batch_size, height, width) and you want to comnpute mean and variance per pixel instead of per width"],"metadata":{"id":"Njvllmj-EDpT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Gradient Clipping"],"metadata":{"id":"wQOzrto8FWVl"}},{"cell_type":"markdown","source":["- `clipvalue=1.0` -> optimizer will clip components of gradient to range [-1.0, 1.0] (may change direction of gradient)\n","- `clipnorm=1.0` -> clips norm of gradient to 1.0 (preserves direction of gradient)"],"metadata":{"id":"gAFG9_xmFmsn"}},{"cell_type":"code","source":["# just an example of an optimizer\n","\n","rmsprop = tf.keras.optimzirs.RMSprop(clipnorm=1.0, clipvalue=1.0) # default = None"],"metadata":{"id":"z0wPsmXsFlOF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Transfer Learning"],"metadata":{"id":"QvhPLl5XGj1e"}},{"cell_type":"markdown","source":["- Suppose we have a model trained on a similar task\n","- Note: model_A and model_B_on_A now share layers -> if you train model_B_on_A -> model_A will also be affected"],"metadata":{"id":"eZAuQN-kHFGm"}},{"cell_type":"code","source":["# Load existing model\n","model_A = tf.keras.models.load_model(\"my_model_A\")\n","# Remove output layer\n","model_B_on_A = tf.keras.Sequential(model_A.layers[:-1])\n","# Add new output layer\n","model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"],"metadata":{"id":"Z23JCVNzGlZ6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`Clone models`\n","\n","- clone_model() only copies architecture\n","- use set_weights() and get_weights() to copy them"],"metadata":{"id":"2d3i-Rx6HQ-D"}},{"cell_type":"code","source":["model_A_clone = tf.keras.models.clone_model(model_A)\n","model_A_clone.set_weights(model_A.get_weights())\n","model_B_on_A = tf.keras.Sequential(model_A_clone.layers[:-1])\n","model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"],"metadata":{"id":"IvuEu0vHHSqh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`Freeze Reused Layers`"],"metadata":{"id":"rWx7AV5uHsYc"}},{"cell_type":"markdown","source":["Note: You must compile the model after freezing / unfreezing layers"],"metadata":{"id":"omJvfpG6IKuF"}},{"cell_type":"code","source":["for layer in model_B_on_A.layers[:-1]:\n","  layer.trainable = False\n","\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.001) # just an example\n","model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"],"metadata":{"id":"Hu6D5qxxHyaq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`Unfreeze reused layers`\n","\n","- After training for a few epochs -> optionally unfreeze (some) layers\n"],"metadata":{"id":"yXpMQOJdIOLF"}},{"cell_type":"code","source":["history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\n","\n","for layer in model_B_on_A.layers[:-1]:\n","  layer.trainable = True\n","\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n","model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n","\n","history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))"],"metadata":{"id":"7lJq0nkcIdBA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Regularization"],"metadata":{"id":"SL71n0PNJMJH"}},{"cell_type":"markdown","source":["`l1 and l2`\n","\n","L2:\n","\n","- Adds regularization term to cost function equal to sum of squared values of weights, multiplied by a constant (0.01 in this case)\n","- $Cost + \\frac{C}{2} ∑_i{w_i^2}$\n","  - with $C$ = constant\n","  - with $w_i$ = weights\n","- Don't use with Adam optimizer -> use AdamW instead\n","\n","L1:\n","\n","- Adds regularization term to cost function equal to the sum of absolute values of weights, multiplied by a constant (0.01 in this case)\n","  - $Cost + C \\sum_i |w_i|$\n","    - with $C$ = constant\n","    - with $w_i$ = weights\n","- Use L1 for sparse models\n","\n","L1_L2:\n","\n","- $Cost + C_1 \\sum_i |w_i| + \\frac{C_2}{2} \\sum_i w_i^2$\n","    - with $C_1$ = constant of L1\n","    - with $C_2$ = constant of L2\n","    - with $w_i$ = weights"],"metadata":{"id":"-whtDO5cJO4o"}},{"cell_type":"code","source":["kr1 = tf.keras.regularizer.l1(0.01)\n","kr2 = tf.keras.regularizer.l2(0.01)\n","kr1_2 = tf.keras.regularizer.l1_l2(l1=0.01, l2=0.01)\n","\n","layer = tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\", kernel_regularizer=kr1)"],"metadata":{"id":"P0xL2tobJOMl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Use partial to avoid repetition"],"metadata":{"id":"EceIREdUMOnH"}},{"cell_type":"code","source":["from functools import partial\n","\n","RegularizedDense = partial(tf.keras.layers.Dense,\n","                            activation=\"relu\",\n","                            kernel_initializer=\"he_normal\",\n","                            kernel_regularizer=tf.keras.regularizers.l2(0.01)\n","                           )\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=[28,28]),\n","    RegularizedDense(100),\n","    RegularizedDense(100),\n","    RegularizedDense(10, activation=\"softmax\")\n","])"],"metadata":{"id":"OOLxS4SXMQqw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dropout in Keras"],"metadata":{"id":"TwHiuxldM-jg"}},{"cell_type":"markdown","source":["- Only does something during training (not during interference)\n","- If model is overfitting -> increase dropout rate and vice versa"],"metadata":{"id":"vT7miSjfNM0Y"}},{"cell_type":"code","source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=[28, 28]),\n","\n","    tf.keras.layers.Dropout(rate=0.2),\n","    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n","\n","    tf.keras.layers.Dropout(rate=0.2),\n","    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n","\n","    tf.keras.layers.Dropout(rate=0.2),\n","    tf.keras.layers.Dense(10, activation=\"softmax\")\n","])"],"metadata":{"id":"FZlU1KfoNABE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`Monte Carlo (MC) Dropout`"],"metadata":{"id":"HQZOZ1BCOScZ"}},{"cell_type":"code","source":["# suppose you have a model called model\n","# with Dropout layers and you want to use MC Dropout\n","\n","class MCDropout(tf.keras.layers.Dropout):\n","    def call(self, inputs):\n","        return super().call(inputs, training=True)\n","\n","Dropout = tf.keras.layers.Dropout\n","mc_model = tf.keras.Sequential([\n","    MCDropout(layer.rate) if isinstance(layer, Dropout) else layer\n","    for layer in model.layers\n","])\n","\n","mc_model.set_weights(model.get_weights())"],"metadata":{"id":"5rkSkLteOWeH"},"execution_count":null,"outputs":[]}]}