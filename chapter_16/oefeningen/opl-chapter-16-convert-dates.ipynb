{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"x7FBsqZaiGyK"},"outputs":[],"source":["!pip install faker"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKAtaw4dfkJk"},"outputs":[],"source":["import faker\n","import random\n","import tqdm\n","import babel.dates\n","\n","fake = faker.Faker()\n","faker.Faker.seed(12345)\n","random.seed(12345)\n","\n","# Define format of the data we would like to generate\n","FORMATS = ['short',\n","           'medium',\n","           'long',\n","           'full',\n","           'full',\n","           'full',\n","           'full',\n","           'full',\n","           'full',\n","           'full',\n","           'full',\n","           'full',\n","           'full',\n","           'd MMM YYY',\n","           'd MMMM YYY',\n","           'dd MMM YYY',\n","           'd MMM, YYY',\n","           'd MMMM, YYY',\n","           'dd, MMM YYY',\n","           'd MM YY',\n","           'd MMMM YYY',\n","           'MMMM d YYY',\n","           'MMMM d, YYY',\n","           'dd.MM.YY']\n","\n","# change this if you want it to work with another language\n","LOCALES = ['nl_NL', 'de_DE']\n","\n","\n","def load_date():\n","    \"\"\"\n","        Loads some fake dates\n","        :returns: tuple containing human readable string, machine readable string, and date object\n","    \"\"\"\n","    dt = fake.date_object()\n","\n","    try:\n","        human_readable = babel.dates.format_date(dt, format=random.choice(FORMATS),\n","                                     locale=random.choice(LOCALES)) # locale=random.choice(LOCALES))\n","        human_readable = human_readable.lower()\n","        human_readable = human_readable.replace(',','')\n","        machine_readable = dt.isoformat()\n","\n","    except AttributeError as e:\n","        return None, None, None\n","\n","    return human_readable, machine_readable, dt\n","\n","\n","def load_dataset(m):\n","    \"\"\"\n","        Loads a dataset with m examples and vocabularies\n","        :m: the number of examples to generate\n","    \"\"\"\n","\n","    dataset = []\n","\n","    for i in range(m):\n","        h, m, _ = load_date()\n","        if h is not None:\n","            dataset.append((h, m))\n","\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nFIafIg1gSzE"},"outputs":[],"source":["m = 20_000\n","dataset = load_dataset(m)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zeF4EDvlhoMz"},"outputs":[],"source":["dataset[:20]"]},{"cell_type":"markdown","metadata":{"id":"sFP-YgdHjFu7"},"source":["### Start of the assignment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NE5RaKNrjI6z"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"UMMwRivojPoK"},"source":["Chop original dataset in three parts. First 10000 for training,\n","next 5000 for validation and last 5000 for testing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ZZiLJO9jlaz"},"outputs":[],"source":["train = dataset[:10_000]\n","valid = dataset[10_000:15_000]\n","test = dataset[15_000:20_000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHDJXWSileZB"},"outputs":[],"source":["train_human, train_machine = zip(*train)\n","valid_human, valid_machine = zip(*valid)\n","test_human, test_machine = zip(*test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFsOxkRkl4kK"},"outputs":[],"source":["train_human[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFPkBBurjupC"},"outputs":[],"source":["human_text_vec_layer = tf.keras.layers.TextVectorization(\n","    split=\"character\",\n","    standardize=None\n",")\n","human_text_vec_layer.adapt(train_human)\n","print(human_text_vec_layer.get_vocabulary())\n","print(len(human_text_vec_layer.get_vocabulary()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QCBVGEWmZPx"},"outputs":[],"source":["machine_text_vec_layer = tf.keras.layers.TextVectorization(\n","    split=\"character\",\n","    standardize=None\n",")\n","machine_text_vec_layer.adapt(train_machine)\n","print(machine_text_vec_layer.get_vocabulary())\n","print(len(machine_text_vec_layer.get_vocabulary()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DkGhNUUzmjW5"},"outputs":[],"source":["human_text_vec_layer([\"29 oktober 2023\", \"1 mei 1978 ??\"])"]},{"cell_type":"markdown","metadata":{"id":"1KHn-DcmnXNH"},"source":["Een eerste gemakkelijk model.\n","\n","- GRU model voor encoder. Vector als uitvoer.\n","- Deze vector invoeren als iedere stap bij de decoder.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2liRKeSEpdGQ"},"outputs":[],"source":["X_train = human_text_vec_layer(train_human)\n","y_train = machine_text_vec_layer(train_machine)\n","X_valid = human_text_vec_layer(valid_human)\n","y_valid = machine_text_vec_layer(valid_machine)\n","X_test = human_text_vec_layer(test_human)\n","y_test = machine_text_vec_layer(test_machine)\n","X_train.shape, y_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t7Di9eNtPMai"},"outputs":[],"source":["def get_model_1(input_vocab_size,\n","                output_vocab_size,\n","                output_seq_length=10,\n","                embedding_size=16,\n","                recurrent_units=64):\n","\n","  encoder = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(\n","        input_dim=input_vocab_size,\n","        output_dim=embedding_size,\n","        mask_zero=True),\n","    tf.keras.layers.GRU(units=recurrent_units),\n","  ])\n","\n","  decoder = tf.keras.Sequential([\n","    tf.keras.layers.GRU(units=recurrent_units, return_sequences=True),\n","    tf.keras.layers.Dense(\n","        units=output_vocab_size,\n","        activation=\"softmax\"\n","    )\n","  ])\n","\n","  model = tf.keras.Sequential([\n","    encoder,\n","    tf.keras.layers.RepeatVector(output_seq_length),\n","    decoder\n","  ])\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JlhAeFtNr5F6"},"outputs":[],"source":["tf.keras.backend.clear_session()\n","model = get_model_1(\n","    input_vocab_size=len(human_text_vec_layer.get_vocabulary()),\n","    output_vocab_size=len(machine_text_vec_layer.get_vocabulary()),\n",")\n","\n","model.compile(\n","    loss=\"sparse_categorical_crossentropy\",\n","    optimizer=\"adam\",\n","    metrics=[\"accuracy\"]\n",")\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zp6kxtkSsE_R"},"outputs":[],"source":["model.fit(X_train, y_train, batch_size=128, epochs=20, validation_data=(X_valid, y_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXx0-hA2t3Bx"},"outputs":[],"source":["datums = [\"1 mei 2023\", \"zondag 29 oktober 2023\"]\n","human_text_vec_layer(datums)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"myxR2ED1uJqu"},"outputs":[],"source":["def convert_dates(model, dates):\n","  # Model: verwacht (batch, seq_length) als invoer.\n","  #        Retourneert, (batch, seq_out_length, num_out_tokens)\n","  # Datums: lijst van strings met input datums\n","  model_proba_predictions = model(human_text_vec_layer(dates))\n","  model_predictions = tf.math.argmax(model_proba_predictions, axis=-1) # (batch, seq_out_length)\n","  machine_vocabulary = np.asarray(machine_text_vec_layer.get_vocabulary())\n","  return [''.join(row) for row in machine_vocabulary[model_predictions]]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FhPPemw4vCqM"},"outputs":[],"source":["datums = [\"1 mei 2023\", \"zondag 29 oktober 2023\"]\n","convert_dates(model, datums)"]},{"cell_type":"markdown","metadata":{"id":"JHWf4gtIx3yj"},"source":["## Tweede model\n","\n","Bekijk het als een vertaalprobleem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1DKuHlsA0Yu8"},"outputs":[],"source":["machine_trslt_text_vec_layer = tf.keras.layers.TextVectorization(\n","    split=\"character\",\n","    standardize=None\n",")\n","machine_trslt_text_vec_layer.adapt([\".\" + date + \"*\" for date in train_machine])\n","print(machine_trslt_text_vec_layer.get_vocabulary())\n","print(len(machine_trslt_text_vec_layer.get_vocabulary()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDAjNx0J0vxM"},"outputs":[],"source":["X_train = tf.constant(train_human)\n","X_valid = tf.constant(valid_human)\n","\n","X_train_dec = tf.constant([\".\" + date for date in train_machine])\n","X_valid_dec = tf.constant([\".\" + date for date in valid_machine])\n","\n","Y_train_dec = machine_trslt_text_vec_layer(tf.constant([date + \"*\" for date in train_machine]))\n","Y_valid_dec = machine_trslt_text_vec_layer(tf.constant([date + \"*\" for date in valid_machine]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9NG3YB-oIGr"},"outputs":[],"source":["def get_model_2(encoder_vectorizer_layer,\n","                decoder_vectorizer_layer,\n","                embedding_size=16,\n","                recurrent_units=64):\n","\n","  # Define input layers\n","  encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string, name=\"enc_input\")\n","  decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string, name=\"dec_input\")\n","\n","  # Vectorize the strings\n","  encoder_input_ids = encoder_vectorizer_layer(encoder_inputs)\n","  decoder_input_ids = decoder_vectorizer_layer(decoder_inputs)\n","\n","  # Define the embedding layers that sit in front of the encoder and the decoder\n","  encoder_embedding_layer = tf.keras.layers.Embedding(\n","      input_dim=len(encoder_vectorizer_layer.get_vocabulary()),\n","      output_dim=embedding_size,\n","      mask_zero=True,\n","      name=\"enc_embed\"\n","  )\n","\n","  decoder_embedding_layer = tf.keras.layers.Embedding(\n","      input_dim=len(decoder_vectorizer_layer.get_vocabulary()),\n","      output_dim=embedding_size,\n","      mask_zero=True,\n","      name=\"dec_embed\"\n","  )\n","\n","  # Apply the embedding layers to the integer identifiers of the tokens\n","  encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n","  decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n","\n","  # Define the RNN using GRU cells. Make sure that the encoder returns the state\n","  # Note: when using a GRU the state and the output are the same but this is\n","  # not the case if we would switch to an LSTM\n","  encoder = tf.keras.layers.GRU(units=recurrent_units,\n","                                return_state=True,\n","                                name=\"enc_rnn\")\n","\n","  # Call the RNN on the encoder_embeddings\n","  encoder_outputs, encoder_state = encoder(encoder_embeddings)\n","\n","  # Define the RNN for the decoder. Make sure that this one returns sequences\n","  # as we want to predict the next character at each time step\n","  decoder = tf.keras.layers.GRU(units=recurrent_units,\n","                                return_sequences=True,\n","                                name=\"dec_rnn\")\n","\n","  # Call the decoder with the decoder embeddings as input and the final state\n","  # of the encoder as the initial state\n","  decoder_outputs = decoder(inputs=decoder_embeddings, initial_state=encoder_state)\n","\n","  # Define (time-distributed) fully connected layer.\n","  output_layer = tf.keras.layers.Dense(\n","      units=len(decoder_vectorizer_layer.get_vocabulary()),\n","      activation='softmax',\n","      name=\"fc_out\"\n","      )\n","\n","  # Apply the dense layer to each decoder output\n","  # in order to predict the next character.\n","  Y_proba = output_layer(decoder_outputs)\n","\n","  # Define the model. It has two inputs and one output\n","  model = tf.keras.Model(\n","    inputs=[encoder_inputs, decoder_inputs],\n","    outputs=[Y_proba]\n","  )\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yL5j13CLqpMh"},"outputs":[],"source":["model2 = get_model_2(\n","    encoder_vectorizer_layer=human_text_vec_layer,\n","    decoder_vectorizer_layer=machine_trslt_text_vec_layer\n",")\n","\n","model2.compile(loss=\"sparse_categorical_crossentropy\",\n","              optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","model2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j11xniKb_b0Q"},"outputs":[],"source":["model2.fit(\n","    (X_train, X_train_dec),\n","    Y_train_dec,\n","    epochs=30,\n","    batch_size=32,\n","    validation_data=((X_valid, X_valid_dec), Y_valid_dec)\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ts3m1bUaD0tO"},"outputs":[],"source":["def convert_date_2(model2, date):\n","  # date should be a string\n","  # Convert a single date\n","\n","  vocabulary = np.asarray(machine_trslt_text_vec_layer.get_vocabulary())\n","\n","  encoder_inputs = tf.constant([date])\n","  current_string = \".\" # start of sequence token\n","  last_predicted_character = ''\n","  i = 0\n","  while last_predicted_character != '*': # End of sequence token\n","    current_string += last_predicted_character\n","    decoder_inputs = tf.constant([current_string])\n","    y_probas = model2([encoder_inputs, decoder_inputs])\n","    y_last_char_ids = tf.math.argmax(y_probas[:, -1], axis=-1)\n","    last_chars = vocabulary[y_last_char_ids]\n","    last_predicted_character = last_chars[0] # only one item in batch\n","\n","  return current_string[1:]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2t-e9f_MFVR1"},"outputs":[],"source":["convert_date_2(model2, \"1 januari 1970\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}