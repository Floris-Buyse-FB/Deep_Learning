{"cells":[{"cell_type":"markdown","id":"10dfffda-f71f-4fce-85dc-458b3a851739","metadata":{"id":"10dfffda-f71f-4fce-85dc-458b3a851739"},"source":["## Generating (Shakespearean) Text with a Transformer (Decoder)"]},{"cell_type":"code","execution_count":null,"id":"503456c2-377a-4b3b-86c6-5768c7f9a7fd","metadata":{"tags":[],"id":"503456c2-377a-4b3b-86c6-5768c7f9a7fd"},"outputs":[],"source":["# Suppress tensorflow warnings\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"]},{"cell_type":"code","execution_count":null,"id":"59bff261-bdad-4393-ac2e-e5674068e3d8","metadata":{"tags":[],"id":"59bff261-bdad-4393-ac2e-e5674068e3d8"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf"]},{"cell_type":"markdown","id":"55c26f84-7b18-47d4-9af5-8ace9d619109","metadata":{"id":"55c26f84-7b18-47d4-9af5-8ace9d619109"},"source":["## Create a GPT Like Decoder Block"]},{"cell_type":"code","execution_count":null,"id":"5ef45b13-c451-47fc-b224-930600bb63a9","metadata":{"id":"5ef45b13-c451-47fc-b224-930600bb63a9"},"outputs":[],"source":["class GPTDecoderBlock(tf.keras.layers.Layer):\n","    \"\"\"\n","    This class implements the \"Decoder\" block from the \"Attention is all You Need\" paper,\n","    but is doesn't include the multi-head (cross) attention because there is no encoder.\n","    \"\"\"\n","\n","    def __init__(self, num_heads, embed_size, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.num_heads = num_heads\n","        self.embed_size = embed_size\n","\n","        self.masked_attn_layer = tf.keras.layers.MultiHeadAttention(\n","            num_heads=num_heads,\n","            key_dim=embed_size // self.num_heads, # Correct ?? See section 3.2.2 of \"attention is all you need\" paper\n","        )\n","\n","        self.norm1_layer = tf.keras.layers.LayerNormalization()\n","\n","        self.dense1_layer = tf.keras.layers.Dense(\n","            units=4*embed_size,\n","            activation=\"relu\") # See section 3.3 of \"attention is all you need\" paper\n","        # No activation function on second dense layer\n","        self.dense2_layer = tf.keras.layers.Dense(units=embed_size)\n","\n","        self.norm2_layer = tf.keras.layers.LayerNormalization()\n","\n","\n","\n","    def call(self, inputs):\n","\n","        # Masked Multi-Head (Self)-Attention block\n","        skip = inputs\n","        inputs = self.masked_attn_layer(\n","            query=inputs,\n","            value=inputs,\n","            use_causal_mask=True)\n","        inputs = self.norm1_layer(\n","            tf.keras.layers.Add()([inputs, skip]))\n","\n","        # Feedforward block\n","        skip = inputs\n","        inputs = self.dense1_layer(inputs)\n","        inputs = self.dense2_layer(inputs)\n","\n","        inputs = self.norm2_layer(\n","            tf.keras.layers.Add()([inputs, skip]))\n","\n","        return inputs\n",""]},{"cell_type":"code","execution_count":null,"id":"d0c6fb03-8608-4b6e-8d0b-7b66f1e0c39d","metadata":{"tags":[],"id":"d0c6fb03-8608-4b6e-8d0b-7b66f1e0c39d"},"outputs":[],"source":["## Test the block, e.g. by testing the output for an input\n","## of shape (2, 10, 64)\n","block = GPTDecoderBlock(num_heads=4, embed_size=64)\n","X = tf.constant(0., shape=(2, 10, 64))\n","block(X).shape"]},{"cell_type":"markdown","id":"86c670e9-a864-4c9a-81b7-22c52e36e933","metadata":{"id":"86c670e9-a864-4c9a-81b7-22c52e36e933"},"source":["## Prepare the Data for Shakespeare Text Generation"]},{"cell_type":"code","execution_count":null,"id":"f97e9139-ae39-4a66-ae06-70905542c869","metadata":{"tags":[],"id":"f97e9139-ae39-4a66-ae06-70905542c869"},"outputs":[],"source":["SEQ_LENGTH = 100"]},{"cell_type":"code","execution_count":null,"id":"dd6e6051-bb3f-4b0f-90fa-45abe9f7fc25","metadata":{"tags":[],"id":"dd6e6051-bb3f-4b0f-90fa-45abe9f7fc25"},"outputs":[],"source":["# Fetch the data, same as in book\n","shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n","filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n","with open(filepath) as f:\n","    shakespeare_text = f.read() # shakespeare_text is now a string"]},{"cell_type":"code","execution_count":null,"id":"04a84bc0-3ed2-4b0f-b596-092792532402","metadata":{"tags":[],"id":"04a84bc0-3ed2-4b0f-b596-092792532402"},"outputs":[],"source":["# Split on characters, keep punctuation as well as upper- and lowercase letters\n","text_vec_layer = tf.keras.layers.TextVectorization(\n","  split=\"character\", standardize=None) # also keep upper case etc.\n","# shakespeare_text is a string and adapt expects a dataset or list\n","text_vec_layer.adapt([shakespeare_text])\n","encoded = text_vec_layer([shakespeare_text])[0]"]},{"cell_type":"code","execution_count":null,"id":"c05dc98a-a223-4f11-962a-b75b1eb35de2","metadata":{"tags":[],"id":"c05dc98a-a223-4f11-962a-b75b1eb35de2"},"outputs":[],"source":["print(text_vec_layer.get_vocabulary())\n","print(len(text_vec_layer.get_vocabulary()))"]},{"cell_type":"code","execution_count":null,"id":"a298dda6-dc61-4fb5-b218-df9b81b5a709","metadata":{"tags":[],"id":"a298dda6-dc61-4fb5-b218-df9b81b5a709"},"outputs":[],"source":["encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\n","              # use broadcasting to subtract 2 from all values\n","n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 65\n","dataset_size = len(encoded)  # total number of chars = 1,115,394"]},{"cell_type":"code","execution_count":null,"id":"ef58aff8-09be-4ba4-ac46-ad11de6fa660","metadata":{"tags":[],"id":"ef58aff8-09be-4ba4-ac46-ad11de6fa660"},"outputs":[],"source":["# create sequence to sequence dataset, same as in book\n","def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n","    ds = tf.data.Dataset.from_tensor_slices(sequence)\n","    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n","    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n","    if shuffle:\n","        ds = ds.shuffle(100_000, seed=seed)\n","    ds = ds.batch(batch_size)\n","    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"]},{"cell_type":"code","execution_count":null,"id":"f9464247-b4f9-417a-9b7b-9f1849618487","metadata":{"tags":[],"id":"f9464247-b4f9-417a-9b7b-9f1849618487"},"outputs":[],"source":["# Create training, validation and test data set. Same as in book.\n","tf.random.set_seed(42)\n","train_set = to_dataset(encoded[:1_000_000], length=SEQ_LENGTH, shuffle=True,\n","                       seed=42)\n","valid_set = to_dataset(encoded[1_000_000:1_060_000], length=SEQ_LENGTH)\n","test_set = to_dataset(encoded[1_060_000:], length=SEQ_LENGTH)"]},{"cell_type":"markdown","id":"3161bb8b-be6e-44bf-b025-f688b3587455","metadata":{"id":"3161bb8b-be6e-44bf-b025-f688b3587455"},"source":["## Create the model."]},{"cell_type":"code","execution_count":null,"id":"14d234a7-e4b8-402c-82cc-b0d06fac500e","metadata":{"tags":[],"id":"14d234a7-e4b8-402c-82cc-b0d06fac500e"},"outputs":[],"source":["class GPTModel(tf.keras.Model):\n","\n","    def __init__(self, n_tokens, embed_size, num_blocks, num_heads, max_seq_length, **kwargs):\n","\n","        super().__init__(**kwargs)\n","\n","        self.num_heads = num_heads\n","        self.max_seq_length = max_seq_length\n","\n","        # Layers\n","        self.embed_layer = tf.keras.layers.Embedding(\n","            input_dim=n_tokens,\n","            output_dim=embed_size,\n","            name='embedding')\n","        self.pos_embed_layer = tf.keras.layers.Embedding(\n","            input_dim=max_seq_length,\n","            output_dim=embed_size,\n","            name='positional_embedding')\n","        #self.add_layer = tf.keras.layers.Add()\n","        self.decoder_blocks = [GPTDecoderBlock(\n","            num_heads=num_heads,\n","            embed_size=embed_size,\n","            name='GPTBlock' + str(i)) for i in range(num_blocks)]\n","        self.dense_layer = tf.keras.layers.Dense(\n","            units=n_tokens,\n","            activation='softmax',\n","            name='output')\n","\n","    def call(self, inputs):\n","\n","        embeddings = self.embed_layer(inputs)\n","\n","        pos_embeddings = self.pos_embed_layer(tf.range(self.max_seq_length))\n","\n","\n","        embeddings = embeddings + pos_embeddings # Rely on broadcasting\n","\n","        for decoder_block in self.decoder_blocks:\n","            embeddings = decoder_block(embeddings)\n","\n","        output = self.dense_layer(embeddings)\n","\n","        return output"]},{"cell_type":"markdown","id":"0ed7260d-e55b-480d-a23c-d08978f25c0b","metadata":{"id":"0ed7260d-e55b-480d-a23c-d08978f25c0b"},"source":["## Instantiate a Model and Train it"]},{"cell_type":"code","execution_count":null,"id":"596ffd59-9da1-493c-8cf2-a86f87822e07","metadata":{"tags":[],"id":"596ffd59-9da1-493c-8cf2-a86f87822e07"},"outputs":[],"source":["EMBED_SIZE = 32\n","NUM_HEADS = 4\n","NUM_BLOCKS = 2"]},{"cell_type":"code","execution_count":null,"id":"84708b92-68d0-49a9-913e-99d4c32f94fc","metadata":{"tags":[],"id":"84708b92-68d0-49a9-913e-99d4c32f94fc"},"outputs":[],"source":["tf.keras.backend.clear_session()\n","model = GPTModel(n_tokens=n_tokens,\n","                  embed_size=EMBED_SIZE,\n","                  num_blocks=NUM_BLOCKS,\n","                  num_heads=NUM_HEADS,\n","                  max_seq_length=SEQ_LENGTH\n","                 )"]},{"cell_type":"code","execution_count":null,"id":"734c384b-e65c-4b40-9bfa-4e43c90a10f2","metadata":{"tags":[],"id":"734c384b-e65c-4b40-9bfa-4e43c90a10f2"},"outputs":[],"source":["for X, Y in train_set.take(1):\n","    print(X.shape)\n","    print(model(X).shape)"]},{"cell_type":"code","execution_count":null,"id":"3f3ebc7b-d7b4-4467-8a31-1bf8cdcdeef6","metadata":{"tags":[],"id":"3f3ebc7b-d7b4-4467-8a31-1bf8cdcdeef6"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"id":"e77fe9d1-c414-4b7b-8b2d-b934eec6504f","metadata":{"tags":[],"id":"e77fe9d1-c414-4b7b-8b2d-b934eec6504f"},"outputs":[],"source":["model.compile(\n","    optimizer='nadam',\n","    loss='sparse_categorical_crossentropy',\n","    metrics=[\"accuracy\"]\n",")"]},{"cell_type":"code","execution_count":null,"id":"bf0f88ed-045c-442d-a55e-5a4482b8cf81","metadata":{"tags":[],"id":"bf0f88ed-045c-442d-a55e-5a4482b8cf81"},"outputs":[],"source":["# 10 minutes per epoch on a (fast) GPU\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss',\n","    patience=2,\n","    restore_best_weights=True\n",")\n","model.fit(train_set, validation_data=valid_set, epochs=20, callbacks=[early_stopping])"]},{"cell_type":"markdown","id":"b02f1e06-9376-492a-b379-026c1f38afe1","metadata":{"id":"b02f1e06-9376-492a-b379-026c1f38afe1"},"source":["## Generating Text"]},{"cell_type":"code","execution_count":null,"id":"9627b99f-394f-4a45-a098-d8ec2e18b591","metadata":{"tags":[],"id":"9627b99f-394f-4a45-a098-d8ec2e18b591"},"outputs":[],"source":["# Create a model that includes the textvectorization layer (same as in book)\n","shakespeare_model = tf.keras.Sequential([\n","  text_vec_layer,\n","  tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n","  model\n","])"]},{"cell_type":"markdown","id":"0555a019-22f0-4ca7-b8fd-e0df270da2be","metadata":{"id":"0555a019-22f0-4ca7-b8fd-e0df270da2be"},"source":["Check the shape of the model when trying to make predictions for one sentence.\n","\n","Note: you can only give the model sentences that have length exactly equal to the length that was used when training the model.\n","This is because we always used this length when computing the positional embeddings."]},{"cell_type":"code","execution_count":null,"id":"e921b74e-3e64-421a-b85f-ade6ea8a03e3","metadata":{"tags":[],"id":"e921b74e-3e64-421a-b85f-ade6ea8a03e3"},"outputs":[],"source":["y_proba = shakespeare_model.predict([\"HAMLET\" + ' '*(SEQ_LENGTH-6)])\n","y_proba.shape"]},{"cell_type":"markdown","id":"b5613bc7-e245-4bad-929d-9b5b03928929","metadata":{"id":"b5613bc7-e245-4bad-929d-9b5b03928929"},"source":["Adapt the `next_char` method from the book so that it works for the transformer model we trained."]},{"cell_type":"code","execution_count":null,"id":"8d17e808-ebbe-42d8-9590-f56266b6ab90","metadata":{"tags":[],"id":"8d17e808-ebbe-42d8-9590-f56266b6ab90"},"outputs":[],"source":["def next_char(text, temperature=1):\n","    # shakepeare_model is the model we trained earlier\n","    y_proba = shakespeare_model.predict([text + ' ' * (SEQ_LENGTH-len(text))])[0, len(text) - 1:len(text)]\n","    rescaled_logits = tf.math.log(y_proba) / temperature\n","    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n","    # text_vec_layer is the layer we adapted earlier\n","    return text_vec_layer.get_vocabulary()[char_id + 2]"]},{"cell_type":"code","execution_count":null,"id":"4f558adf-c569-485c-b03c-f98019397d45","metadata":{"tags":[],"id":"4f558adf-c569-485c-b03c-f98019397d45"},"outputs":[],"source":["next_char(\"to be or not to be \")"]},{"cell_type":"markdown","id":"d075fc1f-7de2-4de2-8517-78218bda2145","metadata":{"id":"d075fc1f-7de2-4de2-8517-78218bda2145"},"source":["Adapt the `extend_text` method from the book.\n","\n","We should be able to generate texts of arbitrary lengths.  Make sure to try your method when predicting at least one hundred characters."]},{"cell_type":"code","execution_count":null,"id":"18f6de2e-9e30-4161-b08c-37612c796b42","metadata":{"tags":[],"id":"18f6de2e-9e30-4161-b08c-37612c796b42"},"outputs":[],"source":["def extend_text(text, n_chars=50, temperature=1):\n","    for _ in range(n_chars):\n","        text += next_char(text[-SEQ_LENGTH:], temperature)\n","    return text"]},{"cell_type":"code","execution_count":null,"id":"5f8aba08-3cb5-4853-ae5b-a73c636376be","metadata":{"tags":[],"id":"5f8aba08-3cb5-4853-ae5b-a73c636376be"},"outputs":[],"source":["extend_text(\"HAMLET:\", n_chars=100, temperature=0.2)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}