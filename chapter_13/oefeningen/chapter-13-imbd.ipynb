{"cells":[{"cell_type":"markdown","metadata":{"id":"gMfMWBSOluzn"},"source":["## IMDB with Datasets and Preprocessing Layers\n","\n","Solution to exercise about unprocessed IMDB reviews."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehVxMEiIGAjm"},"outputs":[],"source":["import os\n","import shutil\n","import random\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_nGrsy4GVus"},"outputs":[],"source":["file_path = tf.keras.utils.get_file(\n","  origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n","  extract=True,\n","  cache_dir=\".\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"24TZFAAVHUBj"},"source":["### Step 2: Create validation directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpnwTL7xGzaT"},"outputs":[],"source":["# Create validation directories\n","os.makedirs(\"datasets/aclImdb/val/pos\", exist_ok=True)\n","os.makedirs(\"datasets/aclImdb/val/neg\", exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eyXO0HdAIpXd"},"outputs":[],"source":["neg_reviews = os.listdir(\"datasets/aclImdb/test/neg\")\n","random.shuffle(neg_reviews)\n","pos_reviews = os.listdir(\"datasets/aclImdb/test/pos\")\n","random.shuffle(pos_reviews)\n","\n","# Use assert to prevent moving files twice\n","assert len(neg_reviews) == 12500 and len(pos_reviews) == 12500\n","for file_name in neg_reviews[:7500]:\n","  shutil.move(\"datasets/aclImdb/test/neg/\" + file_name,\n","              \"datasets/aclImdb/val/neg/\")\n","\n","for file_name in pos_reviews[:7500]:\n","  shutil.move(\"datasets/aclImdb/test/pos/\" + file_name,\n","              \"datasets/aclImdb/val/pos/\")"]},{"cell_type":"markdown","metadata":{"id":"ejk8SMSHJTB6"},"source":["## Step 3: Create tf.data.Dataset objects"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNfC1aAbJIH7"},"outputs":[],"source":["def create_dataset_1(base_dir):\n","  # Approach 1: read all reviews into a list and use from_tensor_slices.\n","  # base_dir: directory name like \"./datasets/aclImdb/train\"\n","  reviews = []\n","  sentiments = []\n","  for sentiment in [\"pos\", \"neg\"]:\n","    directory = os.path.join(base_dir, sentiment)\n","    for file_path in os.listdir(directory):\n","      with open(os.path.join(directory, file_path), \"r\") as file:\n","        reviews.append(file.readlines())\n","      sentiments.append(1.0 if sentiment == \"pos\" else 0.0)\n","\n","  return tf.data.Dataset.from_tensor_slices((reviews, sentiments))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1XoOEkPlJBLr"},"outputs":[],"source":["for X, y in create_dataset_1(\"./datasets/aclImdb/test\").take(3):\n","    print(X)\n","    print(y)\n","    print(\"*\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-lukq2HMBOA"},"outputs":[],"source":["def create_dataset_2(base_dir):\n","  pos_file_paths = [os.path.join(base_dir, \"pos\", f) for f in os.listdir(os.path.join(base_dir, \"pos\"))]\n","  neg_file_paths = [os.path.join(base_dir, \"neg\", f) for f in os.listdir(os.path.join(base_dir, \"neg\"))]\n","\n","  pos_ds = tf.data.TextLineDataset(pos_file_paths, num_parallel_reads=4).map(lambda review : (review, 1.0))\n","  neg_ds = tf.data.TextLineDataset(neg_file_paths, num_parallel_reads=4).map(lambda review : (review, 0.0))\n","\n","  return pos_ds.concatenate(neg_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8BnKIiD-OHWg"},"outputs":[],"source":["for X, y in create_dataset_2(\"./datasets/aclImdb/test\").take(3):\n","    print(X)\n","    print(y)\n","    print(\"*\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfxNsY2aOX_Q"},"outputs":[],"source":["%timeit -r1 for X, y in create_dataset_1(\"./datasets/aclImdb/train\"): pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oxn5yRq5PPAI"},"outputs":[],"source":["%timeit -r1 for X, y in create_dataset_2(\"./datasets/aclImdb/train\"): pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mOCG9LxBOyiQ"},"outputs":[],"source":["BATCH_SIZE=512\n","train_ds = (\n","    create_dataset_2(\"./datasets/aclImdb/train\").\n","    shuffle(buffer_size=15_000, seed=42).\n","    batch(BATCH_SIZE).\n","    prefetch(1)\n",")\n","val_ds = (\n","    create_dataset_2(\"./datasets/aclImdb/val\").\n","    batch(BATCH_SIZE).\n","    prefetch(1)\n",")\n","test_ds = (\n","    create_dataset_2(\"./datasets/aclImdb/test\").\n","    batch(BATCH_SIZE).\n","    prefetch(1)\n",")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"i28BVqwsPtRH"},"source":["### Step 4: Create and train a model with multi-hot encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9w3zA2wSPzlI"},"outputs":[],"source":["VOCAB_SIZE=10_000\n","multi_hot_layer = tf.keras.layers.TextVectorization(\n","    max_tokens=VOCAB_SIZE,\n","    output_mode=\"multi_hot\"\n",")\n","multi_hot_layer.adapt(train_ds.map(lambda review, sentiment : review))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-Hr19-oRktA"},"outputs":[],"source":["multi_hot_layer([\"This movie was great\", \"Terrible!\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gTIh7dXGRwd2"},"outputs":[],"source":["def get_model(conversion_layer):\n","    model = tf.keras.Sequential()\n","\n","    model.add(conversion_layer)\n","\n","    model.add(tf.keras.layers.Dense(units=16, activation='relu',\n","                                    kernel_initializer=\"he_uniform\"))\n","    model.add(tf.keras.layers.Dense(units=16, activation='relu',\n","                                    kernel_initializer=\"he_uniform\"))\n","    model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10csTdanSA0G"},"outputs":[],"source":["early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor=\"val_accuracy\",\n","    min_delta=0.001,\n","    patience=5,\n","    restore_best_weights=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9mco1nFESZj-"},"outputs":[],"source":["model = get_model(multi_hot_layer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6FWlNGQSX1o"},"outputs":[],"source":["model.compile(\n","    optimizer='rmsprop',\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnxzNQyaSjpm"},"outputs":[],"source":["history = model.fit(\n","    train_ds,\n","    validation_data = val_ds,\n","    epochs=100,\n","    callbacks=[early_stopping],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBljjt9hT7bW"},"outputs":[],"source":["def plot_learning_curves(history):\n","    plt.figure(figsize=(8, 5))\n","    for key, style in zip(history.history, [\"r-o\", \"r-*\", \"b-o\", \"b-*\"]):\n","        epochs = np.array(history.epoch)\n","        plt.plot(epochs + 1, history.history[key], style, label=key)\n","    plt.xlabel(\"Epoch\")\n","    plt.axis([1, len(history.history['loss']), 0., 1])\n","    plt.legend(loc=\"lower left\")\n","    plt.grid()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8999BYasUNgm"},"outputs":[],"source":["plot_learning_curves(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AOxJfdMZUXl-"},"outputs":[],"source":["model.evaluate(val_ds)"]},{"cell_type":"markdown","metadata":{"id":"G7XYivsnTxvW"},"source":["### Step 5: Create and Train a Model with TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFJmrBKmTzuQ"},"outputs":[],"source":["tf_idf_layer = tf.keras.layers.TextVectorization(\n","    max_tokens=VOCAB_SIZE,\n","    output_mode=\"tf_idf\"\n",")\n","tf_idf_layer.adapt(train_ds.map(lambda review, sentiment : review))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qVcX7lubTdne"},"outputs":[],"source":["model2 = get_model(tf_idf_layer)\n","model2.compile(\n","    optimizer='rmsprop',\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")\n","history = model2.fit(\n","    train_ds,\n","    validation_data = val_ds,\n","    epochs=100,\n","    callbacks=[early_stopping],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9au11k3Wf9N"},"outputs":[],"source":["model2.evaluate(val_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8qOfyJjb8zL"},"outputs":[],"source":["model2.summary()"]},{"cell_type":"markdown","metadata":{"id":"x5KU9f2MVm29"},"source":["### Step 6: Create a Custom Embedding Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRRPrDnsVrNW"},"outputs":[],"source":["int_layer = tf.keras.layers.TextVectorization(\n","    max_tokens = VOCAB_SIZE,\n","    output_mode = 'int'\n",")\n","int_layer.adapt(train_ds.map(lambda review, sentiment : review))\n","int_layer(['It was a terrible movie', \"Super!\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDQKo55bYUh0"},"outputs":[],"source":["class MeanEmbeddingLayer(tf.keras.layers.Layer):\n","\n","  def __init__(self, input_dim, output_dim, **kwargs):\n","    super().__init__(**kwargs)\n","    self.input_dim = input_dim\n","    self.output_dim = output_dim\n","    self.embedding_layer = tf.keras.layers.Embedding(\n","        input_dim=self.input_dim,\n","        output_dim=self.output_dim\n","    )\n","\n","  def call(self, ints):\n","    ## ints should be (batch_size, max_sequence_length)\n","\n","    #  multiplier.shape -> (ints.shape[0], self.output_dim, 1)\n","    multiplier = tf.expand_dims(tf.where(ints != 0, 1.0, 0.0), -1)\n","\n","\n","    # scale_factor.shape -> (ints.shape[0], 1),\n","    scale_factor = tf.math.sqrt(\n","        tf.math.count_nonzero(\n","            ints, axis=-1, keepdims=True, dtype=tf.dtypes.float32)\n","        )\n","    # word_embedding.shape -> batch_size, max_seq_length, output_dim)\n","    word_embeddings = self.embedding_layer(ints)\n","\n","    # unscaled_sum.shape -> (ints.shape[0], self.output_dim)\n","    unscaled_sum = tf.reduce_sum(word_embeddings * multiplier, axis=-2)\n","\n","    return unscaled_sum / scale_factor\n","\n","  def get_config(self):\n","        base_config = super().get_config()\n","        return {**base_config, \"input_dim\": self.input_dim,\n","                               \"output_dim\": self.output_dim}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KET766hYbdSU"},"outputs":[],"source":["def get_model_with_embedding(vectorization_layer, output_dim):\n","  # vectorization layer should be a TextVectorization layer\n","  # output_dim is the dimensionality of the embedding vectors\n","\n","  model = tf.keras.Sequential()\n","\n","  model.add(vectorization_layer)\n","\n","  model.add(MeanEmbeddingLayer(len(vectorization_layer.get_vocabulary()),\n","                               output_dim))\n","\n","  model.add(tf.keras.layers.Dense(units=16, activation='relu',\n","                                  kernel_initializer=\"he_uniform\"))\n","  model.add(tf.keras.layers.Dense(units=16, activation='relu',\n","                                  kernel_initializer=\"he_uniform\"))\n","  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","  return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OO8D8i7IcgMa"},"outputs":[],"source":["int_layer = tf.keras.layers.TextVectorization(\n","    max_tokens = VOCAB_SIZE,\n","    output_mode = 'int'\n",")\n","int_layer.adapt(train_ds.map(lambda review, sentiment : review))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTsueNmIcnYs"},"outputs":[],"source":["model3 = get_model_with_embedding(int_layer, output_dim=16)\n","model3.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9V3z242jdYgZ"},"outputs":[],"source":["model3.compile(\n","    optimizer='rmsprop',\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")\n","history = model3.fit(\n","    train_ds,\n","    validation_data = val_ds,\n","    epochs=100,\n","    callbacks=[early_stopping],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMI-vHeYe50Z"},"outputs":[],"source":["model3.evaluate(val_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VoKP0E3BrSfs"},"outputs":[],"source":["model3.evaluate(test_ds)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}